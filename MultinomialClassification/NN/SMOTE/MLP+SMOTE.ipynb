{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# STEP 1 â€” LOAD DATA\n",
        "# =====================================================\n",
        "\n",
        "train_path = \"/content/drive/MyDrive/ML_Project/MultinomialClassification/Dataset/train.csv\"\n",
        "test_path  = \"/content/drive/MyDrive/ML_Project/MultinomialClassification/Dataset/test.csv\"\n",
        "\n",
        "\n",
        "\n",
        "df_raw = pd.read_csv(train_path)\n",
        "test_raw = pd.read_csv(test_path)\n",
        "\n",
        "TARGET = \"spend_category\"\n",
        "ID_COL = \"trip_id\"\n",
        "\n",
        "print(\"Original Training Shape:\", df_raw.shape)\n",
        "print(\"Original Test Shape:\", test_raw.shape)\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# HELPER â€” Convert Range (15-30, 90+) to numeric\n",
        "# =====================================================\n",
        "def range_to_mid(x):\n",
        "    x = str(x).strip()\n",
        "    if x.lower() in [\"none\", \"\", \"nan\", \"null\"]:\n",
        "        return np.nan\n",
        "    if \"+\" in x:\n",
        "        return float(x.replace(\"+\", \"\"))\n",
        "    if \"-\" in x:\n",
        "        a, b = x.split(\"-\")\n",
        "        return (float(a) + float(b)) / 2\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# STEP 2 â€” GLOBAL COLUMN DEFINITIONS\n",
        "# =====================================================\n",
        "binary_cols = [\n",
        "    \"is_first_visit\",\"intl_transport_included\",\"accomodation_included\",\n",
        "    \"food_included\",\"domestic_transport_included\",\"sightseeing_included\",\n",
        "    \"guide_included\",\"insurance_included\"\n",
        "]\n",
        "\n",
        "categorical_cols = [\n",
        "    \"country\",\"age_group\",\"travel_companions\",\"main_activity\",\n",
        "    \"visit_purpose\",\"tour_type\",\"info_source\",\"arrival_weather\"\n",
        "]\n",
        "\n",
        "numeric_count_cols = [\"num_females\",\"num_males\",\"mainland_stay_nights\",\"island_stay_nights\"]\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# STEP 3 â€” REMOVE NULL TARGETS FIRST\n",
        "# =====================================================\n",
        "removed_target_nulls = df_raw[TARGET].isnull().sum()\n",
        "print(\"Rows removed due to null spend_category:\", removed_target_nulls)\n",
        "\n",
        "df_raw = df_raw[df_raw[TARGET].notnull()].reset_index(drop=True)\n",
        "print(\"Training shape after removing null targets:\", df_raw.shape)\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# STEP 4 â€” MAIN PREPROCESSING FUNCTION\n",
        "# =====================================================\n",
        "def preprocess_raw_df(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Clean strings\n",
        "    for c in df.columns:\n",
        "        if df[c].dtype == object:\n",
        "            df[c] = df[c].astype(str).str.strip().str.rstrip(',')\n",
        "\n",
        "    # Binary processing\n",
        "    for c in binary_cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = df[c].astype(str).str.strip().str.lower()\n",
        "            df[c] = df[c].replace({\n",
        "                \"yes\": 1,\n",
        "                \"no\": 0,\n",
        "                \"nan\": np.nan,\n",
        "                \"none\": np.nan,\n",
        "                \"null\": np.nan,\n",
        "                \"\": np.nan\n",
        "            })\n",
        "            df[c] = df[c].fillna(0).astype(int)\n",
        "\n",
        "    # Numeric count fields\n",
        "    for c in numeric_count_cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "    # =====================================================\n",
        "    # SAFE ORDINAL ENCODING FOR RANGE COLUMNS\n",
        "    # =====================================================\n",
        "\n",
        "    # Clean weird string values\n",
        "    def clean_str(x):\n",
        "        x = str(x).strip().lower()\n",
        "        if x in [\"nan\", \"none\", \"null\", \"\"]:\n",
        "            return np.nan\n",
        "        return x\n",
        "\n",
        "    # Clean raw string columns\n",
        "    if \"days_booked_before_trip\" in df.columns:\n",
        "        df[\"days_booked_before_trip_clean\"] = df[\"days_booked_before_trip\"].apply(clean_str)\n",
        "\n",
        "    if \"total_trip_days\" in df.columns:\n",
        "        df[\"total_trip_days_clean\"] = df[\"total_trip_days\"].apply(clean_str)\n",
        "\n",
        "    # Define ordinal mappings\n",
        "    ordinal_days_booked = {\n",
        "        \"1-7\": 1,\n",
        "        \"8-14\": 2,\n",
        "        \"15-30\": 3,\n",
        "        \"31-60\": 4,\n",
        "        \"61-90\": 5,\n",
        "        \"90+\": 6\n",
        "    }\n",
        "\n",
        "    ordinal_total_trip = {\n",
        "        \"1-6\": 1,\n",
        "        \"7-14\": 2,\n",
        "        \"15-30\": 3,\n",
        "        \"30+\": 4\n",
        "    }\n",
        "\n",
        "    # Map â†’ Fill Missing â†’ Convert to int\n",
        "    if \"days_booked_before_trip_clean\" in df.columns:\n",
        "        df[\"days_booked_before_trip_ord\"] = (\n",
        "            df[\"days_booked_before_trip_clean\"]\n",
        "                .map(ordinal_days_booked)\n",
        "        )\n",
        "\n",
        "        # Fill NaN with mode **of ordinal values**\n",
        "        df[\"days_booked_before_trip_ord\"].fillna(\n",
        "            df[\"days_booked_before_trip_ord\"].mode()[0],\n",
        "            inplace=True\n",
        "        )\n",
        "\n",
        "        df[\"days_booked_before_trip_ord\"] = df[\"days_booked_before_trip_ord\"].astype(int)\n",
        "\n",
        "    if \"total_trip_days_clean\" in df.columns:\n",
        "        df[\"total_trip_days_ord\"] = (\n",
        "            df[\"total_trip_days_clean\"]\n",
        "                .map(ordinal_total_trip)\n",
        "        )\n",
        "\n",
        "        # Fill NaN with mode of ordinal values\n",
        "        df[\"total_trip_days_ord\"].fillna(\n",
        "            df[\"total_trip_days_ord\"].mode()[0],\n",
        "            inplace=True\n",
        "        )\n",
        "\n",
        "        df[\"total_trip_days_ord\"] = df[\"total_trip_days_ord\"].astype(int)\n",
        "\n",
        "\n",
        "\n",
        "    # Special requirements â†’ binary\n",
        "    if \"has_special_requirements\" in df.columns:\n",
        "        df[\"has_special_req_bin\"] = df[\"has_special_requirements\"].astype(str).apply(\n",
        "            lambda x: 0 if x.lower() in [\"none\", \"\", \"nan\"] else 1\n",
        "        )\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# APPLY PREPROCESSING TO TRAIN & TEST\n",
        "# =====================================================\n",
        "df = preprocess_raw_df(df_raw).reset_index(drop=True)\n",
        "test_df = preprocess_raw_df(test_raw).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nAfter Base Preprocessing:\")\n",
        "print(df.shape)\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# STEP 5 â€” IMPUTATIONS (NO NaNs must remain)\n",
        "# =====================================================\n",
        "\n",
        "# Categorical mode fill\n",
        "for c in categorical_cols:\n",
        "    if c in df.columns:\n",
        "        mode = df[c].mode()[0]\n",
        "        df[c] = df[c].fillna(mode)\n",
        "        test_df[c] = test_df[c].fillna(mode)\n",
        "\n",
        "# =========================================================\n",
        "# OUTLIER REMOVAL (TRAIN ONLY)\n",
        "# And count number of rows removed per condition\n",
        "# =========================================================\n",
        "\n",
        "clean_df = df.copy()\n",
        "initial_rows = len(clean_df)\n",
        "\n",
        "outlier_info = {}\n",
        "\n",
        "# ----- num_females â‰¤ 14 -----\n",
        "before = len(clean_df)\n",
        "clean_df = clean_df[clean_df[\"num_females\"] <= 10]\n",
        "after = len(clean_df)\n",
        "outlier_info[\"num_females\"] = before - after\n",
        "\n",
        "# ----- Rule 1: num_males â‰¤ 13 -----\n",
        "before = len(clean_df)\n",
        "clean_df = clean_df[clean_df[\"num_males\"] <= 10]\n",
        "after_rule1 = len(clean_df)\n",
        "removed_rule1 = before - after_rule1\n",
        "before_rule2 = len(clean_df)\n",
        "\n",
        "# Save results\n",
        "outlier_info[\"num_males_threshold\"] = removed_rule1\n",
        "print(\"Removed (num_males > 20):\", removed_rule1)\n",
        "\n",
        "\n",
        "# ----- mainland_stay_nights â‰¤ 100 -----\n",
        "before = len(clean_df)\n",
        "clean_df = clean_df[clean_df[\"mainland_stay_nights\"] <= 90]\n",
        "after = len(clean_df)\n",
        "outlier_info[\"mainland_stay_nights\"] = before - after\n",
        "\n",
        "# ----- island_stay_nights â‰¤ 21 -----\n",
        "before = len(clean_df)\n",
        "clean_df = clean_df[clean_df[\"island_stay_nights\"] <= 60]\n",
        "after = len(clean_df)\n",
        "outlier_info[\"island_stay_nights\"] = before - after\n",
        "\n",
        "final_rows = len(clean_df)\n",
        "\n",
        "# =========================================================\n",
        "# PRINT OUTLIER REMOVAL SUMMARY\n",
        "# =========================================================\n",
        "print(\"\\n========== OUTLIER REMOVAL SUMMARY ==========\")\n",
        "for col, removed in outlier_info.items():\n",
        "    print(f\"{col}: removed {removed} rows\")\n",
        "\n",
        "print(\"---------------------------------------------\")\n",
        "print(f\"Total rows removed: {initial_rows - final_rows}\")\n",
        "print(f\"Final Training Shape after Outlier Removal: {clean_df.shape}\")\n",
        "print(\"Test Shape (unchanged):\", test_df.shape)\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# STEP 5 â€” FINAL FEATURE DEFINITIONS (No NaNs left)\n",
        "# =====================================================\n",
        "\n",
        "numeric_features = [\n",
        "    \"num_females\",\n",
        "    \"num_males\",\n",
        "    \"mainland_stay_nights\",\n",
        "    \"island_stay_nights\",\n",
        "    \"days_booked_before_trip_ord\",\n",
        "    \"total_trip_days_ord\"\n",
        "]\n",
        "\n",
        "binary_features = binary_cols + [\"has_special_req_bin\"]\n",
        "\n",
        "categorical_features = categorical_cols\n",
        "\n",
        "all_features = numeric_features + binary_features + categorical_features\n",
        "\n",
        "clean_df = clean_df[all_features + [TARGET]]\n",
        "test_df_final = test_df[all_features]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gIiQDu4T-Sqm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8494b6c-0fd6-42dc-808e-af670d16113e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Training Shape: (12654, 25)\n",
            "Original Test Shape: (5852, 24)\n",
            "Rows removed due to null spend_category: 34\n",
            "Training shape after removing null targets: (12620, 25)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4121578799.py:92: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[c] = df[c].replace({\n",
            "/tmp/ipython-input-4121578799.py:92: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[c] = df[c].replace({\n",
            "/tmp/ipython-input-4121578799.py:92: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[c] = df[c].replace({\n",
            "/tmp/ipython-input-4121578799.py:92: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[c] = df[c].replace({\n",
            "/tmp/ipython-input-4121578799.py:92: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[c] = df[c].replace({\n",
            "/tmp/ipython-input-4121578799.py:92: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[c] = df[c].replace({\n",
            "/tmp/ipython-input-4121578799.py:92: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[c] = df[c].replace({\n",
            "/tmp/ipython-input-4121578799.py:92: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[c] = df[c].replace({\n",
            "/tmp/ipython-input-4121578799.py:150: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[\"days_booked_before_trip_ord\"].fillna(\n",
            "/tmp/ipython-input-4121578799.py:164: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[\"total_trip_days_ord\"].fillna(\n",
            "/tmp/ipython-input-4121578799.py:92: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[c] = df[c].replace({\n",
            "/tmp/ipython-input-4121578799.py:92: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[c] = df[c].replace({\n",
            "/tmp/ipython-input-4121578799.py:92: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[c] = df[c].replace({\n",
            "/tmp/ipython-input-4121578799.py:92: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[c] = df[c].replace({\n",
            "/tmp/ipython-input-4121578799.py:92: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[c] = df[c].replace({\n",
            "/tmp/ipython-input-4121578799.py:92: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[c] = df[c].replace({\n",
            "/tmp/ipython-input-4121578799.py:92: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[c] = df[c].replace({\n",
            "/tmp/ipython-input-4121578799.py:92: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df[c] = df[c].replace({\n",
            "/tmp/ipython-input-4121578799.py:150: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[\"days_booked_before_trip_ord\"].fillna(\n",
            "/tmp/ipython-input-4121578799.py:164: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[\"total_trip_days_ord\"].fillna(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After Base Preprocessing:\n",
            "(12620, 30)\n",
            "Removed (num_males > 20): 8\n",
            "\n",
            "========== OUTLIER REMOVAL SUMMARY ==========\n",
            "num_females: removed 28 rows\n",
            "num_males_threshold: removed 8 rows\n",
            "mainland_stay_nights: removed 27 rows\n",
            "island_stay_nights: removed 8 rows\n",
            "---------------------------------------------\n",
            "Total rows removed: 71\n",
            "Final Training Shape after Outlier Removal: (12549, 30)\n",
            "Test Shape (unchanged): (5852, 29)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# =====================================================\n",
        "# STEP X â€” FEATURE ENGINEERING: K-Means + GMM CLUSTERS\n",
        "# =====================================================\n",
        "\n",
        "print(\"\\n=== Running Clustering Feature Engineering (KMeans) ===\")\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Prepare numeric data only\n",
        "# ----------------------------\n",
        "num_only = clean_df[numeric_features].copy()\n",
        "test_num_only = test_df_final[numeric_features].copy()\n",
        "\n",
        "# Standardize for clustering\n",
        "scaler = StandardScaler()\n",
        "num_scaled = scaler.fit_transform(num_only)\n",
        "test_num_scaled = scaler.transform(test_num_only)\n",
        "\n",
        "# ----------------------------\n",
        "# 2. K-Means Clustering\n",
        "# ----------------------------\n",
        "kmeans = KMeans(n_clusters=6, random_state=42, n_init=10)\n",
        "clean_df[\"kmeans_cluster\"] = kmeans.fit_predict(num_scaled)\n",
        "test_df_final[\"kmeans_cluster\"] = kmeans.predict(test_num_scaled)\n",
        "\n",
        "# # ----------------------------\n",
        "# # 3. Gaussian Mixture Model (GMM)\n",
        "# # ----------------------------\n",
        "# gmm = GaussianMixture(n_components=5, random_state=42)\n",
        "# clean_df[\"gmm_cluster\"] = gmm.fit_predict(num_scaled)\n",
        "# test_df_final[\"gmm_cluster\"] = gmm.predict(test_num_scaled)\n",
        "\n",
        "print(\"Clustering features added: kmeans_cluster\")\n",
        "\n",
        "# Add to categorical features\n",
        "categorical_features += [\"kmeans_cluster\"]\n",
        "\n",
        "# Update full feature list\n",
        "all_features = numeric_features + binary_features + categorical_features\n",
        "\n",
        "# Important: keep only final columns\n",
        "clean_df = clean_df[all_features + [TARGET]]\n",
        "test_df_final = test_df_final[all_features]\n",
        "\n",
        "print(\"\\nFinal shape after clustering:\", clean_df.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6CFap8Tu2iD",
        "outputId": "20c237b6-3101-4bf6-a605-6633b08a951e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Running Clustering Feature Engineering (KMeans) ===\n",
            "Clustering features added: kmeans_cluster\n",
            "\n",
            "Final shape after clustering: (12549, 25)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1854590008.py:26: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_df_final[\"kmeans_cluster\"] = kmeans.predict(test_num_scaled)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "preprocess = ColumnTransformer([\n",
        "    (\"num\", StandardScaler(), numeric_features),\n",
        "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_features),\n",
        "    (\"bin\", \"passthrough\", binary_features)\n",
        "])\n"
      ],
      "metadata": {
        "id": "dctFQxbJvMcX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# CORRECT SPLITTING ACCORDING TO YOUR REQUIREMENT\n",
        "# =====================================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ----------- FULL DATASET -----------\n",
        "full_df = clean_df.copy()\n",
        "\n",
        "# =====================================================\n",
        "# SPLIT A: 80% train, 10% val, 10% test\n",
        "# =====================================================\n",
        "\n",
        "# First: 80% train, 20% temp\n",
        "train_80, temp_20 = train_test_split(\n",
        "    full_df,\n",
        "    test_size=0.20,\n",
        "    stratify=full_df[TARGET],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Now split 20% temp â†’ 10% val + 10% test\n",
        "val_10, test_10 = train_test_split(\n",
        "    temp_20,\n",
        "    test_size=0.50,   # half of 20%\n",
        "    stratify=temp_20[TARGET],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Split A (80/10/10):\")\n",
        "print(\"train_80:\", train_80.shape)\n",
        "print(\"val_10:\", val_10.shape)\n",
        "print(\"test_10:\", test_10.shape)\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# SPLIT B: 20% train, 40% val, 40% test\n",
        "# =====================================================\n",
        "\n",
        "# First: 20% train, 80% temp\n",
        "train_20, temp_80 = train_test_split(\n",
        "    full_df,\n",
        "    test_size=0.80,\n",
        "    stratify=full_df[TARGET],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Now split 80% temp â†’ 40% val + 40% test\n",
        "val_40, test_40 = train_test_split(\n",
        "    temp_80,\n",
        "    test_size=0.50,\n",
        "    stratify=temp_80[TARGET],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\nSplit B (20/40/40):\")\n",
        "print(\"train_20:\", train_20.shape)\n",
        "print(\"val_40:\", val_40.shape)\n",
        "print(\"test_40:\", test_40.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL0-lNXsxQWd",
        "outputId": "559a03f6-25d5-456f-adb0-78a8927e1aaa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split A (80/10/10):\n",
            "train_80: (10039, 25)\n",
            "val_10: (1255, 25)\n",
            "test_10: (1255, 25)\n",
            "\n",
            "Split B (20/40/40):\n",
            "train_20: (2509, 25)\n",
            "val_40: (5020, 25)\n",
            "test_40: (5020, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.base import clone\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# =====================================================\n",
        "# MLP Architectures (MULTICLASS)\n",
        "# =====================================================\n",
        "\n",
        "def build_mlp_small(input_dim, num_classes):\n",
        "    return keras.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "def build_mlp_medium(input_dim, num_classes):\n",
        "    return keras.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "def build_mlp_large(input_dim, num_classes):\n",
        "    return keras.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "MLP_BUILDERS = {\n",
        "    \"MLP_SMALL\":  build_mlp_small,\n",
        "    \"MLP_MEDIUM\": build_mlp_medium,\n",
        "    \"MLP_LARGE\":  build_mlp_large,\n",
        "}\n",
        "\n",
        "# NOTE: final Kaggle test features are already in test_df_final[all_features]\n",
        "# and original IDs are in test_raw[ID_COL] or test_df[ID_COL].\n",
        "\n",
        "BASE_NN_DIR = \"/content/drive/MyDrive/ML_Project/MultinomialClassification/NN/SMOTE\"\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# TRAIN ONE MODEL (one size + one split config)\n",
        "# =====================================================\n",
        "def train_mlp_smote(model_name,\n",
        "                    X_train_df, y_train,\n",
        "                    X_val_df,   y_val,\n",
        "                    X_test_int_df, y_test_int,\n",
        "                    run_name):\n",
        "\n",
        "    # Folder structure:\n",
        "    # NN/SMOTE/MLP_SMALL/80_10_10/...\n",
        "    out_dir = os.path.join(BASE_NN_DIR, model_name, run_name)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # Clone ColumnTransformer so each model has its own fitted preprocessor\n",
        "    local_prep = clone(preprocess)\n",
        "    local_prep.fit(X_train_df)\n",
        "\n",
        "    X_train = local_prep.transform(X_train_df)\n",
        "    X_val   = local_prep.transform(X_val_df)\n",
        "    X_test  = local_prep.transform(X_test_int_df)\n",
        "\n",
        "    # SMOTE on training set (MULTICLASS OK)\n",
        "    sm = SMOTE(random_state=SEED)\n",
        "    X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "    input_dim   = X_train_sm.shape[1]\n",
        "    num_classes = len(np.unique(y_train_sm))\n",
        "\n",
        "    # Pick architecture\n",
        "    builder = MLP_BUILDERS[model_name]\n",
        "    model = builder(input_dim, num_classes)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(0.001),\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    es = keras.callbacks.EarlyStopping(\n",
        "        patience=6,\n",
        "        monitor=\"val_loss\",\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    print(f\"\\nğŸš€ Training {model_name} â€” {run_name} ...\")\n",
        "\n",
        "    model.fit(\n",
        "        X_train_sm, y_train_sm,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=30,\n",
        "        batch_size=256,\n",
        "        callbacks=[es],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # ------------- VALIDATION -------------\n",
        "    val_probs = model.predict(X_val)\n",
        "    val_pred  = np.argmax(val_probs, axis=1)\n",
        "    val_acc   = accuracy_score(y_val, val_pred)\n",
        "    val_rep   = classification_report(y_val, val_pred)\n",
        "\n",
        "    with open(os.path.join(out_dir, \"validation_report.txt\"), \"w\") as f:\n",
        "        f.write(val_rep)\n",
        "        f.write(f\"\\nValidation Accuracy: {val_acc}\\n\")\n",
        "\n",
        "    # ------------- INTERNAL TEST -------------\n",
        "    test_probs = model.predict(X_test)\n",
        "    test_pred  = np.argmax(test_probs, axis=1)\n",
        "    test_acc   = accuracy_score(y_test_int, test_pred)\n",
        "    test_rep   = classification_report(y_test_int, test_pred)\n",
        "\n",
        "    with open(os.path.join(out_dir, \"test_report.txt\"), \"w\") as f:\n",
        "        f.write(test_rep)\n",
        "        f.write(f\"\\nTest Accuracy: {test_acc}\\n\")\n",
        "\n",
        "    # ------------- SUMMARY FILE -------------\n",
        "    with open(os.path.join(out_dir, \"summary.txt\"), \"w\") as f:\n",
        "        f.write(f\"Validation Accuracy: {val_acc}\\n\")\n",
        "        f.write(f\"Test Accuracy: {test_acc}\\n\")\n",
        "\n",
        "    # ------------- FINAL KAGGLE SUBMISSION -------------\n",
        "    X_final = local_prep.transform(test_df_final[all_features])\n",
        "    final_probs = model.predict(X_final)\n",
        "    final_pred  = np.argmax(final_probs, axis=1)\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "        ID_COL: test_raw[ID_COL],     # trip_id\n",
        "        TARGET: final_pred            # spend_category\n",
        "    })\n",
        "    submission.to_csv(\n",
        "        os.path.join(out_dir, f\"{model_name}_{run_name}_submission.csv\"),\n",
        "        index=False\n",
        "    )\n",
        "\n",
        "    print(f\"[DONE] {model_name} â€” {run_name} âœ“\\n\")\n",
        "\n",
        "\n",
        "# =====================================================\n",
        "# RUN ALL MLP SIZES ON BOTH SPLITS\n",
        "# =====================================================\n",
        "def run_all_mlps_smote():\n",
        "\n",
        "    # ---- Split A: 80 / 10 / 10 ----\n",
        "    X80   = train_80[all_features]\n",
        "    y80   = train_80[TARGET].values\n",
        "\n",
        "    Xv10  = val_10[all_features]\n",
        "    yv10  = val_10[TARGET].values\n",
        "\n",
        "    Xt10  = test_10[all_features]\n",
        "    yt10  = test_10[TARGET].values\n",
        "\n",
        "    # ---- Split B: 20 / 40 / 40 ----\n",
        "    X20   = train_20[all_features]\n",
        "    y20   = train_20[TARGET].values\n",
        "\n",
        "    Xv40  = val_40[all_features]\n",
        "    yv40  = val_40[TARGET].values\n",
        "\n",
        "    Xt40  = test_40[all_features]\n",
        "    yt40  = test_40[TARGET].values\n",
        "\n",
        "    for model_name in [\"MLP_SMALL\", \"MLP_MEDIUM\", \"MLP_LARGE\"]:\n",
        "        print(f\"\\n============================\")\n",
        "        print(f\" Running: {model_name}\")\n",
        "        print(f\"============================\\n\")\n",
        "\n",
        "        # 80 / 10 / 10\n",
        "        train_mlp_smote(\n",
        "            model_name,\n",
        "            X80, y80,\n",
        "            Xv10, yv10,\n",
        "            Xt10, yt10,\n",
        "            run_name=\"80_10_10\"\n",
        "        )\n",
        "\n",
        "        # 20 / 40 / 40\n",
        "        train_mlp_smote(\n",
        "            model_name,\n",
        "            X20, y20,\n",
        "            Xv40, yv40,\n",
        "            Xt40, yt40,\n",
        "            run_name=\"20_40_40\"\n",
        "        )\n",
        "\n",
        "    print(\"\\nâœ… All SMOTE+MLP runs completed.\\n\")\n",
        "\n",
        "\n",
        "# Actually run everything\n",
        "run_all_mlps_smote()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdOh-aq6vvHg",
        "outputId": "1256bdcd-3174-4552-f89a-a1c7182a0576"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================\n",
            " Running: MLP_SMALL\n",
            "============================\n",
            "\n",
            "\n",
            "ğŸš€ Training MLP_SMALL â€” 80_10_10 ...\n",
            "Epoch 1/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.5171 - loss: 0.9637 - val_accuracy: 0.6693 - val_loss: 0.7090\n",
            "Epoch 2/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6963 - loss: 0.7080 - val_accuracy: 0.6972 - val_loss: 0.6692\n",
            "Epoch 3/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7164 - loss: 0.6611 - val_accuracy: 0.7195 - val_loss: 0.6537\n",
            "Epoch 4/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7262 - loss: 0.6395 - val_accuracy: 0.7124 - val_loss: 0.6475\n",
            "Epoch 5/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7306 - loss: 0.6296 - val_accuracy: 0.7267 - val_loss: 0.6315\n",
            "Epoch 6/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7324 - loss: 0.6223 - val_accuracy: 0.7179 - val_loss: 0.6490\n",
            "Epoch 7/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7434 - loss: 0.6059 - val_accuracy: 0.7195 - val_loss: 0.6600\n",
            "Epoch 8/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7442 - loss: 0.5997 - val_accuracy: 0.7179 - val_loss: 0.6566\n",
            "Epoch 9/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7483 - loss: 0.5970 - val_accuracy: 0.7235 - val_loss: 0.6464\n",
            "Epoch 10/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7563 - loss: 0.5805 - val_accuracy: 0.7275 - val_loss: 0.6542\n",
            "Epoch 11/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7607 - loss: 0.5751 - val_accuracy: 0.7371 - val_loss: 0.6429\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m183/183\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "[DONE] MLP_SMALL â€” 80_10_10 âœ“\n",
            "\n",
            "\n",
            "ğŸš€ Training MLP_SMALL â€” 20_40_40 ...\n",
            "Epoch 1/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.3786 - loss: 1.1315 - val_accuracy: 0.5853 - val_loss: 0.9429\n",
            "Epoch 2/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.5925 - loss: 0.9278 - val_accuracy: 0.6319 - val_loss: 0.8032\n",
            "Epoch 3/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6528 - loss: 0.8009 - val_accuracy: 0.6538 - val_loss: 0.7395\n",
            "Epoch 4/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.6799 - loss: 0.7285 - val_accuracy: 0.6633 - val_loss: 0.7119\n",
            "Epoch 5/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6923 - loss: 0.6927 - val_accuracy: 0.6769 - val_loss: 0.6911\n",
            "Epoch 6/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7291 - loss: 0.6540 - val_accuracy: 0.6896 - val_loss: 0.6811\n",
            "Epoch 7/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7259 - loss: 0.6431 - val_accuracy: 0.6954 - val_loss: 0.6717\n",
            "Epoch 8/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7382 - loss: 0.6152 - val_accuracy: 0.6970 - val_loss: 0.6721\n",
            "Epoch 9/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7508 - loss: 0.5964 - val_accuracy: 0.7006 - val_loss: 0.6680\n",
            "Epoch 10/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7482 - loss: 0.5859 - val_accuracy: 0.7016 - val_loss: 0.6681\n",
            "Epoch 11/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7481 - loss: 0.5840 - val_accuracy: 0.7062 - val_loss: 0.6609\n",
            "Epoch 12/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7595 - loss: 0.5654 - val_accuracy: 0.7076 - val_loss: 0.6609\n",
            "Epoch 13/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7615 - loss: 0.5680 - val_accuracy: 0.7072 - val_loss: 0.6610\n",
            "Epoch 14/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7656 - loss: 0.5572 - val_accuracy: 0.7068 - val_loss: 0.6643\n",
            "Epoch 15/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7627 - loss: 0.5566 - val_accuracy: 0.7070 - val_loss: 0.6596\n",
            "Epoch 16/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7696 - loss: 0.5506 - val_accuracy: 0.7042 - val_loss: 0.6638\n",
            "Epoch 17/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7733 - loss: 0.5465 - val_accuracy: 0.7068 - val_loss: 0.6604\n",
            "Epoch 18/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7716 - loss: 0.5388 - val_accuracy: 0.7062 - val_loss: 0.6612\n",
            "Epoch 19/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.7771 - loss: 0.5265 - val_accuracy: 0.7060 - val_loss: 0.6640\n",
            "Epoch 20/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7899 - loss: 0.5191 - val_accuracy: 0.7082 - val_loss: 0.6644\n",
            "Epoch 21/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7798 - loss: 0.5315 - val_accuracy: 0.7060 - val_loss: 0.6637\n",
            "\u001b[1m157/157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m157/157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\u001b[1m183/183\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "[DONE] MLP_SMALL â€” 20_40_40 âœ“\n",
            "\n",
            "\n",
            "============================\n",
            " Running: MLP_MEDIUM\n",
            "============================\n",
            "\n",
            "\n",
            "ğŸš€ Training MLP_MEDIUM â€” 80_10_10 ...\n",
            "Epoch 1/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5349 - loss: 1.0874 - val_accuracy: 0.6908 - val_loss: 0.8232\n",
            "Epoch 2/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6795 - loss: 0.7426 - val_accuracy: 0.7004 - val_loss: 0.7767\n",
            "Epoch 3/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7037 - loss: 0.6875 - val_accuracy: 0.7155 - val_loss: 0.7361\n",
            "Epoch 4/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7125 - loss: 0.6650 - val_accuracy: 0.7100 - val_loss: 0.7010\n",
            "Epoch 5/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7221 - loss: 0.6476 - val_accuracy: 0.7052 - val_loss: 0.6820\n",
            "Epoch 6/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7446 - loss: 0.6033 - val_accuracy: 0.6972 - val_loss: 0.6889\n",
            "Epoch 7/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7489 - loss: 0.5907 - val_accuracy: 0.6932 - val_loss: 0.6931\n",
            "Epoch 8/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.7511 - loss: 0.5831 - val_accuracy: 0.6964 - val_loss: 0.6763\n",
            "Epoch 9/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7636 - loss: 0.5663 - val_accuracy: 0.7004 - val_loss: 0.6823\n",
            "Epoch 10/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7660 - loss: 0.5612 - val_accuracy: 0.7084 - val_loss: 0.6750\n",
            "Epoch 11/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7792 - loss: 0.5279 - val_accuracy: 0.7100 - val_loss: 0.6707\n",
            "Epoch 12/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7776 - loss: 0.5253 - val_accuracy: 0.7076 - val_loss: 0.6997\n",
            "Epoch 13/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7905 - loss: 0.5113 - val_accuracy: 0.7092 - val_loss: 0.6886\n",
            "Epoch 14/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7929 - loss: 0.5052 - val_accuracy: 0.7227 - val_loss: 0.6803\n",
            "Epoch 15/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8017 - loss: 0.4892 - val_accuracy: 0.7139 - val_loss: 0.7114\n",
            "Epoch 16/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8041 - loss: 0.4738 - val_accuracy: 0.7179 - val_loss: 0.7018\n",
            "Epoch 17/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8102 - loss: 0.4659 - val_accuracy: 0.7307 - val_loss: 0.7038\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m183/183\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "[DONE] MLP_MEDIUM â€” 80_10_10 âœ“\n",
            "\n",
            "\n",
            "ğŸš€ Training MLP_MEDIUM â€” 20_40_40 ...\n",
            "Epoch 1/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - accuracy: 0.4208 - loss: 1.3465 - val_accuracy: 0.6514 - val_loss: 0.9213\n",
            "Epoch 2/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.6660 - loss: 0.8374 - val_accuracy: 0.6938 - val_loss: 0.8637\n",
            "Epoch 3/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7048 - loss: 0.7323 - val_accuracy: 0.6817 - val_loss: 0.8667\n",
            "Epoch 4/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.7154 - loss: 0.6837 - val_accuracy: 0.6996 - val_loss: 0.8381\n",
            "Epoch 5/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7431 - loss: 0.6415 - val_accuracy: 0.7042 - val_loss: 0.8233\n",
            "Epoch 6/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7412 - loss: 0.6054 - val_accuracy: 0.7108 - val_loss: 0.7994\n",
            "Epoch 7/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7622 - loss: 0.5773 - val_accuracy: 0.7054 - val_loss: 0.7900\n",
            "Epoch 8/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7545 - loss: 0.5698 - val_accuracy: 0.7120 - val_loss: 0.7643\n",
            "Epoch 9/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.7696 - loss: 0.5451 - val_accuracy: 0.7104 - val_loss: 0.7444\n",
            "Epoch 10/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7984 - loss: 0.5252 - val_accuracy: 0.7094 - val_loss: 0.7375\n",
            "Epoch 11/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.7984 - loss: 0.5164 - val_accuracy: 0.7098 - val_loss: 0.7227\n",
            "Epoch 12/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8028 - loss: 0.4835 - val_accuracy: 0.7110 - val_loss: 0.7106\n",
            "Epoch 13/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8060 - loss: 0.4854 - val_accuracy: 0.7112 - val_loss: 0.7031\n",
            "Epoch 14/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8089 - loss: 0.4664 - val_accuracy: 0.7078 - val_loss: 0.6996\n",
            "Epoch 15/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8235 - loss: 0.4422 - val_accuracy: 0.7104 - val_loss: 0.6856\n",
            "Epoch 16/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8384 - loss: 0.4043 - val_accuracy: 0.7110 - val_loss: 0.6818\n",
            "Epoch 17/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8328 - loss: 0.4159 - val_accuracy: 0.7120 - val_loss: 0.6749\n",
            "Epoch 18/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8312 - loss: 0.4148 - val_accuracy: 0.7137 - val_loss: 0.6706\n",
            "Epoch 19/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8388 - loss: 0.4018 - val_accuracy: 0.7040 - val_loss: 0.6764\n",
            "Epoch 20/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8624 - loss: 0.3686 - val_accuracy: 0.7110 - val_loss: 0.6661\n",
            "Epoch 21/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8614 - loss: 0.3617 - val_accuracy: 0.7080 - val_loss: 0.6703\n",
            "Epoch 22/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8658 - loss: 0.3356 - val_accuracy: 0.7042 - val_loss: 0.6771\n",
            "Epoch 23/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8611 - loss: 0.3522 - val_accuracy: 0.7102 - val_loss: 0.6761\n",
            "Epoch 24/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8631 - loss: 0.3510 - val_accuracy: 0.7030 - val_loss: 0.6885\n",
            "Epoch 25/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8816 - loss: 0.3118 - val_accuracy: 0.7036 - val_loss: 0.6901\n",
            "Epoch 26/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8850 - loss: 0.3120 - val_accuracy: 0.7040 - val_loss: 0.7019\n",
            "\u001b[1m157/157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "\u001b[1m157/157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m183/183\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "[DONE] MLP_MEDIUM â€” 20_40_40 âœ“\n",
            "\n",
            "\n",
            "============================\n",
            " Running: MLP_LARGE\n",
            "============================\n",
            "\n",
            "\n",
            "ğŸš€ Training MLP_LARGE â€” 80_10_10 ...\n",
            "Epoch 1/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.5262 - loss: 1.1121 - val_accuracy: 0.6335 - val_loss: 0.9299\n",
            "Epoch 2/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6753 - loss: 0.7299 - val_accuracy: 0.6582 - val_loss: 0.8768\n",
            "Epoch 3/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7000 - loss: 0.6873 - val_accuracy: 0.6773 - val_loss: 0.8283\n",
            "Epoch 4/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7247 - loss: 0.6520 - val_accuracy: 0.6853 - val_loss: 0.7905\n",
            "Epoch 5/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7257 - loss: 0.6483 - val_accuracy: 0.6869 - val_loss: 0.7554\n",
            "Epoch 6/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7447 - loss: 0.6065 - val_accuracy: 0.6701 - val_loss: 0.7666\n",
            "Epoch 7/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7428 - loss: 0.6107 - val_accuracy: 0.6900 - val_loss: 0.7323\n",
            "Epoch 8/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7569 - loss: 0.5779 - val_accuracy: 0.7044 - val_loss: 0.7130\n",
            "Epoch 9/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7667 - loss: 0.5671 - val_accuracy: 0.7036 - val_loss: 0.7015\n",
            "Epoch 10/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7685 - loss: 0.5549 - val_accuracy: 0.7139 - val_loss: 0.6888\n",
            "Epoch 11/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7763 - loss: 0.5364 - val_accuracy: 0.7163 - val_loss: 0.7022\n",
            "Epoch 12/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7820 - loss: 0.5210 - val_accuracy: 0.7139 - val_loss: 0.7125\n",
            "Epoch 13/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7952 - loss: 0.4970 - val_accuracy: 0.7068 - val_loss: 0.7143\n",
            "Epoch 14/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.8028 - loss: 0.4831 - val_accuracy: 0.7179 - val_loss: 0.7246\n",
            "Epoch 15/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8071 - loss: 0.4758 - val_accuracy: 0.7211 - val_loss: 0.7355\n",
            "Epoch 16/30\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8143 - loss: 0.4584 - val_accuracy: 0.7211 - val_loss: 0.7233\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m183/183\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "[DONE] MLP_LARGE â€” 80_10_10 âœ“\n",
            "\n",
            "\n",
            "ğŸš€ Training MLP_LARGE â€” 20_40_40 ...\n",
            "Epoch 1/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.4574 - loss: 1.2725 - val_accuracy: 0.6189 - val_loss: 0.9175\n",
            "Epoch 2/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.6295 - loss: 0.8314 - val_accuracy: 0.6655 - val_loss: 0.9102\n",
            "Epoch 3/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6740 - loss: 0.7309 - val_accuracy: 0.6655 - val_loss: 0.8858\n",
            "Epoch 4/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6879 - loss: 0.7052 - val_accuracy: 0.6845 - val_loss: 0.8763\n",
            "Epoch 5/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7170 - loss: 0.6440 - val_accuracy: 0.6950 - val_loss: 0.8418\n",
            "Epoch 6/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7427 - loss: 0.5902 - val_accuracy: 0.6970 - val_loss: 0.8289\n",
            "Epoch 7/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7383 - loss: 0.5916 - val_accuracy: 0.6990 - val_loss: 0.8188\n",
            "Epoch 8/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7531 - loss: 0.5750 - val_accuracy: 0.6884 - val_loss: 0.8007\n",
            "Epoch 9/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7606 - loss: 0.5574 - val_accuracy: 0.6958 - val_loss: 0.7993\n",
            "Epoch 10/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.7724 - loss: 0.5374 - val_accuracy: 0.7018 - val_loss: 0.7684\n",
            "Epoch 11/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.7908 - loss: 0.5086 - val_accuracy: 0.7086 - val_loss: 0.7485\n",
            "Epoch 12/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7932 - loss: 0.4985 - val_accuracy: 0.7082 - val_loss: 0.7430\n",
            "Epoch 13/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8070 - loss: 0.4778 - val_accuracy: 0.6998 - val_loss: 0.7377\n",
            "Epoch 14/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.8184 - loss: 0.4478 - val_accuracy: 0.7056 - val_loss: 0.7145\n",
            "Epoch 15/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.8321 - loss: 0.4274 - val_accuracy: 0.6974 - val_loss: 0.7271\n",
            "Epoch 16/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.8292 - loss: 0.4126 - val_accuracy: 0.7018 - val_loss: 0.7042\n",
            "Epoch 17/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.8316 - loss: 0.4187 - val_accuracy: 0.7042 - val_loss: 0.6993\n",
            "Epoch 18/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.8397 - loss: 0.3919 - val_accuracy: 0.7032 - val_loss: 0.7007\n",
            "Epoch 19/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.8413 - loss: 0.3735 - val_accuracy: 0.7072 - val_loss: 0.6909\n",
            "Epoch 20/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.8541 - loss: 0.3409 - val_accuracy: 0.7102 - val_loss: 0.6950\n",
            "Epoch 21/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.8630 - loss: 0.3490 - val_accuracy: 0.7044 - val_loss: 0.6977\n",
            "Epoch 22/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8582 - loss: 0.3354 - val_accuracy: 0.7094 - val_loss: 0.6971\n",
            "Epoch 23/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.8829 - loss: 0.3027 - val_accuracy: 0.7068 - val_loss: 0.7017\n",
            "Epoch 24/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8756 - loss: 0.3167 - val_accuracy: 0.7026 - val_loss: 0.7201\n",
            "Epoch 25/30\n",
            "\u001b[1m15/15\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8806 - loss: 0.3084 - val_accuracy: 0.7086 - val_loss: 0.7197\n",
            "\u001b[1m157/157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m157/157\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\u001b[1m183/183\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "[DONE] MLP_LARGE â€” 20_40_40 âœ“\n",
            "\n",
            "\n",
            "âœ… All SMOTE+MLP runs completed.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}